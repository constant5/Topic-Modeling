{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy machine learning models to Amazon SageMaker using the ezsmdeploy Python package and a few lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "{'alternate_sign': False, 'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.float64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'n_features': 40000, 'ngram_range': (1, 1), 'norm': 'l2', 'preprocessor': None, 'stop_words': 'english', 'strip_accents': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None}\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import ezsmdeploy\n",
    "import pickle\n",
    "\n",
    "hv = pickle.load(open(os.path.join('LDA','models','hash_vect.pk'), 'rb'))\n",
    "\n",
    "# def data_process(data):\n",
    "#     data = [re.sub('[,\\\\.!?]', '', x) for x in data]\n",
    "#     # Convert the titles to lowercase\n",
    "#     data = [x.lower() for x in data]\n",
    "#     # Remove post with less than 10 words\n",
    "#     data = [x for x in data if len(x.split(' '))>10]\n",
    "#     return hv.transform(data)\n",
    "\n",
    "params = hv.get_params(True)\n",
    "print(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Write a model transform script\n",
    "\n",
    "Make sure you have a ...\n",
    "\n",
    "**\"load_model\" function**\n",
    "\n",
    "* input args are model path\n",
    "* returns loaded model object\n",
    "* model name is the same as what you saved the model file as (see above step)\n",
    "\n",
    "**\"predict\" function**\n",
    "\n",
    "* input args are the loaded model object and a payload\n",
    "* returns the result of model.predict\n",
    "* make sure you format it as a single (or multiple) string return inside a list for real time (for mini batch)\n",
    "* from a client, a list or string or np.array that is sent for prediction is interpreted as bytes. Do what you have to for converting back to list or string or np.array\n",
    "* return the error for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modelscript_sklearn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modelscript_sklearn.py\n",
    "import sklearn\n",
    "from joblib import load\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "\n",
    "hv = pickle.load(open('./hash_vect.pk', 'rb'))\n",
    "\n",
    "#Return loaded model\n",
    "def load_model(modelpath):\n",
    "    print(modelpath)\n",
    "#     clf = load(os.path.join(modelpath,'model.joblib'))\n",
    "    lda = pickle.load(open(os.path.join(modelpath,'lda_model_8.pk'), 'rb'))\n",
    "    print(\"loaded\")\n",
    "    return lda\n",
    "\n",
    "def data_process(data):\n",
    "    data = [re.sub('[,\\\\.!?]', '', x) for x in data]\n",
    "    # Convert the titles to lowercase\n",
    "    data = [x.lower() for x in data]\n",
    "    # Remove post with less than 10 words\n",
    "    data = [x for x in data if len(x.split(' '))>10]\n",
    "    return hv.transform(data)\n",
    "\n",
    "# return prediction based on loaded model (from the step above) and an input payload\n",
    "def predict(model, payload):\n",
    "    try:\n",
    "        # locally, payload may come in as a list\n",
    "        if type(payload)==str:\n",
    "#             payload = data_process(payload)\n",
    "#             payload = hash_vectorize(payload)\n",
    "            out = str(model.transform(data_process([payload]))[0])\n",
    "        # in remote / container based deployment, payload comes in as a stream of bytes\n",
    "        else:\n",
    "#             payload = data_process(paylod.decode())\n",
    "#             payload = hash_vectorize(payload)\n",
    "            out = str(model.transform(data_process([payload.decode()]))[0])\n",
    "    except Exception as e:  \n",
    "        out = [type(payload),str(e)] #useful for debugging!\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does this work locally? (not \"in a container locally\", but actually in local)¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./LDA/models/\n",
      "loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=16384, doc_topic_prior=None,\n",
       "                          evaluate_every=5, learning_decay=0.7,\n",
       "                          learning_method='online', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=8, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=None,\n",
       "                          topic_word_prior=None, total_samples=1000000.0,\n",
       "                          verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modelscript_sklearn import *\n",
    "model = load_model('./LDA/models/')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load some test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Just stay in there, youre done for tonight'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/Reddit.txt') as f:\n",
    "    text = f.readlines()\n",
    "data = [l.replace('\\n','') for l in text]\n",
    "data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model with string input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weightlifter promised his wife to win an Olympic gold medal before she died in a car accident\n",
      "[0.0301322  0.3266709  0.14882721 0.0300641  0.03006834 0.37411071\n",
      " 0.03008058 0.03004597]\n"
     ]
    }
   ],
   "source": [
    "# data = data_process(data)\n",
    "print(data[0])\n",
    "print(predict(model,data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model with byte input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weightlifter promised his wife to win an Olympic gold medal before she died in a car accident\n",
      "[0.0301322  0.3266709  0.14882721 0.0300641  0.03006834 0.37411071\n",
      " 0.03008058 0.03004597]\n"
     ]
    }
   ],
   "source": [
    "# data = data_process(data)\n",
    "print(data[0])\n",
    "print(predict(model,data[0].encode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you have been running other inference containers in local mode, stop existing containers to avoid conflict¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker container stop $(docker container ls -aq) >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Locally\n",
    "\n",
    "Note that to include the serializer vectorizer to the the docker container I had to run this script without `image` parameter and then add `hash_vect.pk` to the `src` folder and run `./src/build-docker.sh 1` to create the `ezsmdeploy-image-1` image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K0:01:48.781782 | compressed model(s)\n",
      "\u001b[K0:01:50.301911 | uploaded model tarball(s) ; check returned modelpath\n",
      "\u001b[K0:01:50.302676 | added requirements file\n",
      "\u001b[K0:01:50.306381 | added source file\n",
      "\u001b[K0:01:50.309202 | added Dockerfile\n",
      "\u001b[K0:01:50.311690 | added model_handler and docker utils\n",
      "\u001b[K0:01:51.583682 | created model(s). Now deploying on local\n",
      "\u001b[32m∙∙∙\u001b[0m \u001b[KAttaching to tmpd_yhynz0_algo-1-jwllp_1\n",
      "\u001b[36malgo-1-jwllp_1  |\u001b[0m Starting the inference server with 2 workers.\n",
      "\u001b[32m∙●∙\u001b[0m \u001b[K\u001b[36malgo-1-jwllp_1  |\u001b[0m 2020/10/26 18:58:09 [crit] 10#10: *1 connect() to unix:/tmp/gunicorn.sock failed (2: No such file or directory) while connecting to upstream, client: 172.20.0.1, server: , request: \"GET /ping HTTP/1.1\", upstream: \"http://unix:/tmp/gunicorn.sock:/ping\", host: \"localhost:8080\"\n",
      "\u001b[36malgo-1-jwllp_1  |\u001b[0m 172.20.0.1 - - [26/Oct/2020:18:58:09 +0000] \"GET /ping HTTP/1.1\" 502 182 \"-\" \"-\"\n",
      "\u001b[36malgo-1-jwllp_1  |\u001b[0m [2020-10-26 18:58:09 +0000] [9] [INFO] Starting gunicorn 20.0.4\n",
      "\u001b[36malgo-1-jwllp_1  |\u001b[0m [2020-10-26 18:58:09 +0000] [9] [INFO] Listening at: unix:/tmp/gunicorn.sock (9)\n",
      "\u001b[36malgo-1-jwllp_1  |\u001b[0m [2020-10-26 18:58:09 +0000] [9] [INFO] Using worker: gevent\n",
      "\u001b[36malgo-1-jwllp_1  |\u001b[0m [2020-10-26 18:58:09 +0000] [13] [INFO] Booting worker with pid: 13\n",
      "\u001b[32m∙∙●\u001b[0m \u001b[K\u001b[36malgo-1-jwllp_1  |\u001b[0m [2020-10-26 18:58:09 +0000] [14] [INFO] Booting worker with pid: 14\n",
      "\u001b[32m●∙∙\u001b[0m \u001b[K\u001b[36malgo-1-jwllp_1  |\u001b[0m /opt/ml/model\n",
      "\u001b[32m∙●∙\u001b[0m \u001b[K\u001b[36malgo-1-jwllp_1  |\u001b[0m loaded\n",
      "!\u001b[36malgo-1-jwllp_1  |\u001b[0m 172.20.0.1 - - [26/Oct/2020:18:58:14 +0000] \"GET /ping HTTP/1.1\" 200 1 \"-\" \"-\"\n",
      "\u001b[K0:02:07.168858 | deployed model\n",
      "\u001b[K0:02:07.169680 | not setting up autoscaling; deploying locally\n",
      "\u001b[K\u001b[32m0:02:07.169960 | Done! ✔\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "ezonsm = ezsmdeploy.Deploy(model = 'LDA/models',\n",
    "                          script = 'modelscript_sklearn.py',\n",
    "                          requirements = ['numpy','scikit-learn==0.22.1'],\n",
    "                          instance_type='local',\n",
    "                          autoscale = True,\n",
    "                          image='ezsmdeploy-image-1',\n",
    "                          wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test containerized version locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-jwllp_1  |\u001b[0m received input data\r\n",
      "\u001b[36malgo-1-jwllp_1  |\u001b[0m b'Weightlifter promised his wife to win an Olympic gold medal before she died in a car accident'\r\n",
      "b'[0.0301322  0.3266709  0.14882721 0.0300641  0.03006834 0.37411071\\n 0.03008058 0.03004597]'\u001b[36malgo-1-jwllp_1  |\u001b[0m predictions from model\r\n",
      "\u001b[36malgo-1-jwllp_1  |\u001b[0m [0.0301322  0.3266709  0.14882721 0.0300641  0.03006834 0.37411071\r\n",
      "\u001b[36malgo-1-jwllp_1  |\u001b[0m  0.03008058 0.03004597]\r\n",
      "\u001b[36malgo-1-jwllp_1  |\u001b[0m 172.20.0.1 - - [26/Oct/2020:18:59:34 +0000] \"POST /invocations HTTP/1.1\" 200 90 \"-\" \"-\"\r\n",
      "\n",
      "\u001b[36malgo-1-jwllp_1  |\u001b[0m received input data\r\n",
      "\u001b[36malgo-1-jwllp_1  |\u001b[0m b'Weightlifter promised his wife to win an Olympic gold medal before she died in a car accident'\r\n",
      "\u001b[36malgo-1-jwllp_1  |\u001b[0m predictions from model\r\n",
      "\u001b[36malgo-1-jwllp_1  |\u001b[0m [0.0301322  0.3266709  0.14882721 0.0300641  0.03006834 0.37411071\r\n",
      "\u001b[36malgo-1-jwllp_1  |\u001b[0m  0.03008058 0.03004597]\r\n",
      "b'[0.0301322  0.3266709  0.14882721 0.0300641  0.03006834 0.37411071\\n 0.03008058 0.03004597]'\u001b[36malgo-1-jwllp_1  |\u001b[0m 172.20.0.1 - - [26/Oct/2020:18:59:34 +0000] \"POST /invocations HTTP/1.1\" 200 90 \"-\" \"-\"\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with str\n",
    "out = ezonsm.predictor.predict(data[0])\n",
    "print(out)\n",
    "# with byte stream\n",
    "out = ezonsm.predictor.predict(data[0].encode())\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-jwllp_1  |\u001b[0m [2020-10-26 19:00:39 +0000] [9] [INFO] Handling signal: term\n",
      "\u001b[36mtmpd_yhynz0_algo-1-jwllp_1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n"
     ]
    }
   ],
   "source": [
    "!docker container stop $(docker container ls -aq) >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy as Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K0:01:47.140494 | compressed model(s)\n",
      "\u001b[K0:01:52.589216 | uploaded model tarball(s) ; check returned modelpath\n",
      "\u001b[K0:01:52.590009 | added requirements file\n",
      "\u001b[K0:01:52.591749 | added source file\n",
      "\u001b[K0:01:52.593101 | added Dockerfile\n",
      "\u001b[K0:01:52.595079 | added model_handler and docker utils\n",
      "\u001b[K0:01:53.713361 | created model(s). Now deploying on ml.t2.medium\n",
      "\u001b[K0:10:27.697488 | deployed model\n",
      "\u001b[K0:10:27.698154 | estimated cost is $0.07 per hour\n",
      "\u001b[K\u001b[32m0:10:27.698257 | Done! ✔\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "ezonsm = ezsmdeploy.Deploy(model = 'LDA/models',\n",
    "                          script = 'modelscript_sklearn.py',\n",
    "                          requirements = ['numpy','scikit-learn==0.22.1'],\n",
    "                          image='629171485058.dkr.ecr.us-east-1.amazonaws.com/ezsmdeploy-image-1',\n",
    "                          instance_type='ml.t2.medium')\n",
    "\n",
    "\n",
    "# ezonsm = ezsmdeploy.Deploy(model = ['model.joblib','model.joblib'], # example of multimodel endpoint. \n",
    "#                            script = 'modelscript_sklearn.py',\n",
    "#                            requirements = ['scikit-learn==0.22.1','numpy',], \n",
    "#                            instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weightlifter promised his wife to win an Olympic gold medal before she died in a car accident\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'[0.0301322  0.3266709  0.14882721 0.0300641  0.03006834 0.37411071\\n 0.03008058 0.03004597]'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data[0])\n",
    "out = ezonsm.predictor.predict(data[0])\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ezsmdeploy[locust]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K0:00:00.001861 | Starting test with Locust\n",
      "\u001b[K0:00:15.074004 | Done! Please see the src folder for locuststats* files\n",
      "\u001b[K"
     ]
    }
   ],
   "source": [
    "ezonsm.test(input_data=data[0].encode(),usercount=20,hatchrate=10,timeoutsecs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Name</th>\n",
       "      <th># requests</th>\n",
       "      <th># failures</th>\n",
       "      <th>Median response time</th>\n",
       "      <th>Average response time</th>\n",
       "      <th>Min response time</th>\n",
       "      <th>Max response time</th>\n",
       "      <th>Average Content Size</th>\n",
       "      <th>Requests/s</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>80%</th>\n",
       "      <th>90%</th>\n",
       "      <th>95%</th>\n",
       "      <th>98%</th>\n",
       "      <th>99%</th>\n",
       "      <th>99.9%</th>\n",
       "      <th>99.99%</th>\n",
       "      <th>99.999</th>\n",
       "      <th>100%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sagemaker</td>\n",
       "      <td>predict</td>\n",
       "      <td>395</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>53</td>\n",
       "      <td>81</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>Aggregated</td>\n",
       "      <td>395</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>53</td>\n",
       "      <td>81</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Type        Name  # requests  # failures  Median response time  \\\n",
       "0  sagemaker     predict         395           0                    17   \n",
       "1       None  Aggregated         395           0                    17   \n",
       "\n",
       "   Average response time  Min response time  Max response time  \\\n",
       "0                     19                 12                112   \n",
       "1                     19                 12                112   \n",
       "\n",
       "   Average Content Size  Requests/s  ...  75%  80%  90%  95%  98%  99%  99.9%  \\\n",
       "0                     0        36.0  ...   20   22   27   35   53   81    110   \n",
       "1                     0        36.0  ...   20   22   27   35   53   81    110   \n",
       "\n",
       "   99.99%  99.999  100%  \n",
       "0     110     110   110  \n",
       "1     110     110   110  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv('src/locuststats_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ezonsm.predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
