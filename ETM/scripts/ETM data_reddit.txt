ETM data_reddit.py output

(pytorch-env) C:\Users\crm0376\Projects\Topic-Modeling\ETM\scripts>python data_reddit.py
reading data...
counting document frequency of words...
building the vocabulary...
  initial vocabulary size: 40469
  vocabulary size after removing stopwords from list: 39983
tokenizing documents and splitting into train/test/valid...
  vocabulary after removing words not in train: 36486
  number of documents (train): 500000 [this should be equal to 500000]
  number of documents (test): 250000 [this should be equal to 250000]
  number of documents (valid): 250000 [this should be equal to 250000]
removing empty documents...
splitting test documents in 2 halves...
creating lists of words...
  len(words_tr):  3478078
  len(words_ts):  1727310
  len(words_ts_h1):  826937
  len(words_ts_h2):  900373
  len(words_va):  1743883
getting doc indices...
  len(np.unique(doc_indices_tr)): 383888 [this should be 383888]
  len(np.unique(doc_indices_ts)): 161055 [this should be 161055]
  len(np.unique(doc_indices_ts_h1)): 161055 [this should be 161055]
  len(np.unique(doc_indices_ts_h2)): 161055 [this should be 161055]
  len(np.unique(doc_indices_va)): 191991 [this should be 191991]
creating bow representation...
splitting bow intro token/value pairs and saving to disk...
Data ready !!