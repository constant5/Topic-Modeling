ETM data_reddit.py output

python data_reddit.py
reading data...
counting document frequency of words...
building the vocabulary...
  initial vocabulary size: 61445
  vocabulary size after removing stopwords from list: 60956
tokenizing documents and splitting into train/test/valid...
  vocabulary after removing words not in train: 60956
  number of documents (train): 2125000 [this should be equal to 2125000]
  number of documents (test): 250000 [this should be equal to 250000]
  number of documents (valid): 125000 [this should be equal to 125000]
removing empty documents...
splitting test documents in 2 halves...
creating lists of words...
  len(words_tr):  23586900
  len(words_ts):  2715866
  len(words_ts_h1):  1309758
  len(words_ts_h2):  1406108
  len(words_va):  1392117
getting doc indices...
  len(np.unique(doc_indices_tr)): 2062919 [this should be 2062919]
  len(np.unique(doc_indices_ts)): 203782 [this should be 203782]
  len(np.unique(doc_indices_ts_h1)): 203782 [this should be 203782]
  len(np.unique(doc_indices_ts_h2)): 203782 [this should be 203782]
  len(np.unique(doc_indices_va)): 121315 [this should be 121315]
creating bow representation...
splitting bow intro token/value pairs and saving to disk...
creating lists of words...
  len(words_tr):  23586900
  len(words_ts):  2715866
  len(words_ts_h1):  1309758
  len(words_ts_h2):  1406108
  len(words_va):  1392117
getting doc indices...
  len(np.unique(doc_indices_tr)): 2062919 [this should be 2062919]
  len(np.unique(doc_indices_ts)): 203782 [this should be 203782]
  len(np.unique(doc_indices_ts_h1)): 203782 [this should be 203782]
  len(np.unique(doc_indices_ts_h2)): 203782 [this should be 203782]
  len(np.unique(doc_indices_va)): 121315 [this should be 121315]
creating bow representation...
splitting bow intro token/value pairs and saving to disk...
Data ready !!
*************

